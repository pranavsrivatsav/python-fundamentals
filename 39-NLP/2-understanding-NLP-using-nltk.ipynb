{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NLP using NLTK\n",
    "#### 2 parts of NLP\n",
    "    - NLU (Natural language understanding)\n",
    "    - NLG (Natural language generation)\n",
    "#### Why NLU is hard?\n",
    "    - Ambiguity? 3 types of ambiguity\n",
    "        - lexical ambiguity: Same word can have different meanings based on the context.\n",
    "            - eg: The lady is looking for a match. (match can be ambiguous)\n",
    "        - syntactic ambiguity: The structure of a sentence can pose challenges in understanding what the sentence is conveying.\n",
    "            - eg: \"I saw the man with the telescope. (Did hee use a telescope to see the man? or Did he see a man using the telescope?)\n",
    "        - referential ambiguity: Sometimes there can be two nouns in a sentence. A determinant (i.e words like a, the) can potentially point to either of the nouns, and the understanding depends on the speaker/listener. This is called referential ambiguity.\n",
    "            - Father met his friend at a shop. He asked for a loan of 10,000 (who asked for a loan?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation and setup of NLTK\n",
    "- install NLTK `py -m pip install nltk` - This will download the basic nltk package but not does not have the necessary tools for us to perform NLP\n",
    "- For that we need to install other packages of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on executing the above function, a window will be opened with different nltk package options that can be individually downloaded.\n",
    "- To keep things simple, let us select the all packages option and click download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "- 3 basic steps involved in tokenization?\n",
    "    - strip: stripping individual words from a sentence.\n",
    "    - understand: understand importance of each word with respect to the sentence\n",
    "    - describe: produce a structural description of the input sentence \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyzing the corpus\n",
    "    - A corpus is nothing but a collection of texts\n",
    "    - NLTK leverage various kinds of Corpora (plural - corpus), and we as a developer can target and make use of the required corpus to perform NLP on our input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'bcp47.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'extended_omw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw-1.4.zip', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pe08', 'pe08.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet2022', 'wordnet2022.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk.corpus\n",
    "\n",
    "#all the corpora available to nltk\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))\n",
    "\n",
    "#the words contained in a corpus called brown\n",
    "print(nltk.corpus.brown.words())\n",
    "\n",
    "#some corpus can have subfiles in them as well\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing our own string\n",
    " - using a word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'words', 'or', 'sentences', '.', 'These', 'smaller', 'units', 'are', 'called', 'tokens', '.', 'Tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'text', 'analysis', '.', 'It', 'helps', 'in', 'understanding', 'the', 'structure', 'and', 'meaning', 'of', 'the', 'text', '.']\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "our_str = \"Tokenization is the process of breaking down text into smaller units, such as words or sentences. These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) and text analysis. It helps in understanding the structure and meaning of the text.\"\n",
    "\n",
    "our_words = word_tokenize(our_str)\n",
    "print(our_words) #you can see that even characters like ',' are also splitted into a separate word\n",
    "\n",
    "print(len(our_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- looking at frequency distribution of words using nltk's FreqDist module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 4, 'the': 3, 'text': 3, 'tokenization': 2, 'is': 2, 'of': 2, 'smaller': 2, 'units': 2, 'in': 2, 'and': 2, ...})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('.', 4),\n",
       " ('the', 3),\n",
       " ('text', 3),\n",
       " ('tokenization', 2),\n",
       " ('is', 2),\n",
       " ('of', 2),\n",
       " ('smaller', 2),\n",
       " ('units', 2),\n",
       " ('in', 2),\n",
       " ('and', 2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fd = FreqDist()\n",
    "\n",
    "for word in our_words:\n",
    "    fd[word.lower()] +=1\n",
    "\n",
    "#frequency distribution of each word\n",
    "display(fd)\n",
    "\n",
    "#we can get the top 10 frequently used words\n",
    "display(fd.most_common(10))\n",
    "\n",
    "#frequency of word and\n",
    "display(fd[\"and\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stripping paras using a blank line tokenizer\n",
    "    - so nltk has different tokenizers, that evaluates tokens based on different logic.\n",
    "    - so, similarly we have a blank line tokenizer, that considers any sequence of blank lines as a delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import blankline_tokenize\n",
    "\n",
    "our_paras = \"\"\"\n",
    "Tokenization is the process of breaking down text into smaller units, such as words or sentences. These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) and text analysis. It helps in understanding the structure and meaning of the text.\n",
    "\n",
    "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "\n",
    "Text analysis involves various techniques to extract information and insights from textual data. These techniques include tokenization, stemming, lemmatization, part-of-speech tagging, named entity recognition, and sentiment analysis. By applying these techniques, we can gain a deeper understanding of the content and context of the text.\n",
    "\"\"\"\n",
    "our_blanks = blankline_tokenize(our_paras)\n",
    "print(len(our_blanks)) #No of blank lines or we can understand them as paras\n",
    "print(our_blanks[1]) #First para\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization via Bigram, Trigram, Ngram\n",
    " - bigrams: tokens created with 2 consecutive words in a sentence\n",
    " - trigrams: tokens with 3 consecutive words in a sentence\n",
    " - ngrams: tokens with n consecutive words in a sentence\n",
    " - They are useful in scenarios like transcribing, translation etc. where we it is easier to chunk n no of words and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'words', 'or', 'sentences', '.', 'These', 'smaller', 'units', 'are', 'called', 'tokens', '.', 'Tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'text', 'analysis', '.', 'It', 'helps', 'in', 'understanding', 'the', 'structure', 'and', 'meaning', 'of', 'the', 'text', '.']\n",
      "[('Tokenization', 'is'), ('is', 'the'), ('the', 'process'), ('process', 'of'), ('of', 'breaking'), ('breaking', 'down'), ('down', 'text'), ('text', 'into'), ('into', 'smaller'), ('smaller', 'units'), ('units', ','), (',', 'such'), ('such', 'as'), ('as', 'words'), ('words', 'or'), ('or', 'sentences'), ('sentences', '.'), ('.', 'These'), ('These', 'smaller'), ('smaller', 'units'), ('units', 'are'), ('are', 'called'), ('called', 'tokens'), ('tokens', '.'), ('.', 'Tokenization'), ('Tokenization', 'is'), ('is', 'a'), ('a', 'fundamental'), ('fundamental', 'step'), ('step', 'in'), ('in', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '('), ('(', 'NLP'), ('NLP', ')'), (')', 'and'), ('and', 'text'), ('text', 'analysis'), ('analysis', '.'), ('.', 'It'), ('It', 'helps'), ('helps', 'in'), ('in', 'understanding'), ('understanding', 'the'), ('the', 'structure'), ('structure', 'and'), ('and', 'meaning'), ('meaning', 'of'), ('of', 'the'), ('the', 'text'), ('text', '.')]\n",
      "[('Tokenization', 'is', 'the'), ('is', 'the', 'process'), ('the', 'process', 'of'), ('process', 'of', 'breaking'), ('of', 'breaking', 'down'), ('breaking', 'down', 'text'), ('down', 'text', 'into'), ('text', 'into', 'smaller'), ('into', 'smaller', 'units'), ('smaller', 'units', ','), ('units', ',', 'such'), (',', 'such', 'as'), ('such', 'as', 'words'), ('as', 'words', 'or'), ('words', 'or', 'sentences'), ('or', 'sentences', '.'), ('sentences', '.', 'These'), ('.', 'These', 'smaller'), ('These', 'smaller', 'units'), ('smaller', 'units', 'are'), ('units', 'are', 'called'), ('are', 'called', 'tokens'), ('called', 'tokens', '.'), ('tokens', '.', 'Tokenization'), ('.', 'Tokenization', 'is'), ('Tokenization', 'is', 'a'), ('is', 'a', 'fundamental'), ('a', 'fundamental', 'step'), ('fundamental', 'step', 'in'), ('step', 'in', 'natural'), ('in', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '('), ('processing', '(', 'NLP'), ('(', 'NLP', ')'), ('NLP', ')', 'and'), (')', 'and', 'text'), ('and', 'text', 'analysis'), ('text', 'analysis', '.'), ('analysis', '.', 'It'), ('.', 'It', 'helps'), ('It', 'helps', 'in'), ('helps', 'in', 'understanding'), ('in', 'understanding', 'the'), ('understanding', 'the', 'structure'), ('the', 'structure', 'and'), ('structure', 'and', 'meaning'), ('and', 'meaning', 'of'), ('meaning', 'of', 'the'), ('of', 'the', 'text'), ('the', 'text', '.')]\n",
      "[('Tokenization', 'is', 'the', 'process', 'of'), ('is', 'the', 'process', 'of', 'breaking'), ('the', 'process', 'of', 'breaking', 'down'), ('process', 'of', 'breaking', 'down', 'text'), ('of', 'breaking', 'down', 'text', 'into'), ('breaking', 'down', 'text', 'into', 'smaller'), ('down', 'text', 'into', 'smaller', 'units'), ('text', 'into', 'smaller', 'units', ','), ('into', 'smaller', 'units', ',', 'such'), ('smaller', 'units', ',', 'such', 'as'), ('units', ',', 'such', 'as', 'words'), (',', 'such', 'as', 'words', 'or'), ('such', 'as', 'words', 'or', 'sentences'), ('as', 'words', 'or', 'sentences', '.'), ('words', 'or', 'sentences', '.', 'These'), ('or', 'sentences', '.', 'These', 'smaller'), ('sentences', '.', 'These', 'smaller', 'units'), ('.', 'These', 'smaller', 'units', 'are'), ('These', 'smaller', 'units', 'are', 'called'), ('smaller', 'units', 'are', 'called', 'tokens'), ('units', 'are', 'called', 'tokens', '.'), ('are', 'called', 'tokens', '.', 'Tokenization'), ('called', 'tokens', '.', 'Tokenization', 'is'), ('tokens', '.', 'Tokenization', 'is', 'a'), ('.', 'Tokenization', 'is', 'a', 'fundamental'), ('Tokenization', 'is', 'a', 'fundamental', 'step'), ('is', 'a', 'fundamental', 'step', 'in'), ('a', 'fundamental', 'step', 'in', 'natural'), ('fundamental', 'step', 'in', 'natural', 'language'), ('step', 'in', 'natural', 'language', 'processing'), ('in', 'natural', 'language', 'processing', '('), ('natural', 'language', 'processing', '(', 'NLP'), ('language', 'processing', '(', 'NLP', ')'), ('processing', '(', 'NLP', ')', 'and'), ('(', 'NLP', ')', 'and', 'text'), ('NLP', ')', 'and', 'text', 'analysis'), (')', 'and', 'text', 'analysis', '.'), ('and', 'text', 'analysis', '.', 'It'), ('text', 'analysis', '.', 'It', 'helps'), ('analysis', '.', 'It', 'helps', 'in'), ('.', 'It', 'helps', 'in', 'understanding'), ('It', 'helps', 'in', 'understanding', 'the'), ('helps', 'in', 'understanding', 'the', 'structure'), ('in', 'understanding', 'the', 'structure', 'and'), ('understanding', 'the', 'structure', 'and', 'meaning'), ('the', 'structure', 'and', 'meaning', 'of'), ('structure', 'and', 'meaning', 'of', 'the'), ('and', 'meaning', 'of', 'the', 'text'), ('meaning', 'of', 'the', 'text', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "our_str = \"Tokenization is the process of breaking down text into smaller units, such as words or sentences. These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) and text analysis. It helps in understanding the structure and meaning of the text.\"\n",
    "tokens = word_tokenize(our_str)\n",
    "print(tokens)\n",
    "tokens_bigrams = bigrams(tokens)\n",
    "tokens_trigrams = trigrams(tokens)\n",
    "tokens_ngrams = ngrams(tokens, 5)\n",
    "print(list(tokens_bigrams))\n",
    "print(list(tokens_trigrams))\n",
    "print(list(tokens_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "- stemming is the process of concentrating a word to its core root word.\n",
    "- This is done by for example removing suffixes like 's','ing' etc. \n",
    "- words like 'affected', 'affects' is stripped to the word affect.\n",
    "- There are various stemmer algorithms like: porterStemmer, lancasterstemmer, snowballstemmer. Let us simulate stemming using these stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affected | porter: affect | lancaster: affect | snowball: affect\n",
      "affecting | porter: affect | lancaster: affect | snowball: affect\n",
      "affects | porter: affect | lancaster: affect | snowball: affect\n",
      "affection | porter: affect | lancaster: affect | snowball: affect\n",
      "gave | porter: gave | lancaster: gav | snowball: gave\n",
      "given | porter: given | lancaster: giv | snowball: given\n",
      "giving | porter: give | lancaster: giv | snowball: give\n",
      "gives | porter: give | lancaster: giv | snowball: give\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "pst, lst, sst = PorterStemmer(), LancasterStemmer(), SnowballStemmer(\"english\")\n",
    "words_to_stem = ['affected', 'affecting', 'affects', 'affection', 'gave', 'given', 'giving', 'gives']\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word} | porter: {pst.stem(word)} | lancaster: {lst.stem(word)} | snowball: {sst.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you see in the above stemming simulation, you can see at times, the stemmer produces an invalid word like 'gav'. So this is a drawback of stemmer. It is fast but can be imprecise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "- What additional things lemmatization does in addition to stemmer?\n",
    "    - it takes the context of the word in a sentence in to account.\n",
    "    - it ensures that the resulting word is a valid word (i.e a word part of a dictionary of the language)\n",
    "- wordnet lemmatization \n",
    "    - wordnet is a popular dictionary, and a lemmatization module is available in nltk written on top of it. Let us see a simulation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affected | affected\n",
      "affecting | affecting\n",
      "affects | affect\n",
      "affection | affection\n",
      "gave | gave\n",
      "given | given\n",
      "giving | giving\n",
      "gives | give\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet, WordNetLemmatizer\n",
    "words_to_stem = ['affected', 'affecting', 'affects', 'affection', 'gave', 'given', 'giving', 'gives']\n",
    "wordnet_lem = WordNetLemmatizer()\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word} | {wordnet_lem.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the way a lemmatizer infers a word is much different than that of a stemmer, and the execution time is far slower. But the result is much more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "- Stop words are words that are part of a language, that are not of much help in the context of natural language processing.\n",
    "- NLTK has corpus which contains stopwords. We can make use of these stop words, to reduce our tokens count.\n",
    "- Let us take a look at stop words in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'such', 't', \"should've\", 'further', 'me', 'here', 'through', 'before', 'nor', 'are', \"don't\", 'does', 'when', \"aren't\", 'there', 'to', 'am', 'into', 'as', 'himself', 'with', 'a', 'ourselves', 'of', 'why', 'any', 'what', 'an', \"you've\", 'under', \"doesn't\", 'our', 'the', 're', 'isn', \"it's\", 'once', 'we', 'hers', 'where', \"mustn't\", 'but', 'd', \"wasn't\", 'for', \"couldn't\", 'only', 'i', 'these', \"she's\", 'so', 'too', 'during', 'have', 'do', 'has', 'some', 'than', \"didn't\", 'because', 'he', 'against', 'them', 'own', 'her', 'then', 'yours', 'being', 'they', 'all', \"needn't\", 'whom', 'weren', 'their', 'be', 'out', 'can', 'yourselves', \"you'd\", 'ain', 'those', 'same', 'down', 'his', \"shouldn't\", 'who', 'from', \"hasn't\", 'which', 'at', 'on', 'until', 'been', 'just', 'doesn', 'more', 'didn', 'itself', 'most', 'wouldn', 'each', 'my', \"hadn't\", 'it', 'in', 'both', 'after', 's', 'over', \"mightn't\", 'and', 'not', 'theirs', 'm', 'now', 'll', \"wouldn't\", 'y', 'yourself', 'ours', \"haven't\", 'how', 'themselves', 'she', 'mustn', 'won', \"you're\", 'ma', 'its', 'below', 'had', 'few', 'will', 'that', 'about', 'again', 'mightn', \"weren't\", \"isn't\", 'was', 'aren', 'hadn', 'herself', 'wasn', 'were', 'did', 'needn', 'other', 'shouldn', 'by', 'haven', 'if', \"shan't\", 'doing', 'off', 'no', 've', \"that'll\", 'is', 'couldn', 'don', 'should', 'above', 'up', \"won't\", 'him', 'very', 'having', 'myself', 'between', 'or', 'you', 'hasn', \"you'll\", 'o', 'your', 'shan', 'while', 'this'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stopwords\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of speech (POS)\n",
    "- In our english classes, do you remember us doing activities like identify the noun, verb, adverb etc from a sentence.\n",
    "- Parts of speech is the same activity part of the nlp process.\n",
    "- Here we process each word, or sometimes a bunch of words and tag them as a particular part of speech identified by a tag id.\n",
    "- These are all of the tags that is used in the POS process:\n",
    "\n",
    "    | Tag  | Description                                      | Example       |\n",
    "    |------|--------------------------------------------------|---------------|\n",
    "    | CC   | Coordinating conjunction                         | and           |\n",
    "    | CD   | Cardinal number                                  | 1, two        |\n",
    "    | DT   | Determiner                                       | the           |\n",
    "    | EX   | Existential there                                | there         |\n",
    "    | FW   | Foreign word                                     | d'autre       |\n",
    "    | IN   | Preposition or subordinating conjunction         | in, of        |\n",
    "    | JJ   | Adjective                                        | quick         |\n",
    "    | JJR  | Adjective, comparative                           | quicker       |\n",
    "    | JJS  | Adjective, superlative                           | quickest      |\n",
    "    | LS   | List item marker                                 | 1., 2.        |\n",
    "    | MD   | Modal                                            | could, will   |\n",
    "    | NN   | Noun, singular or mass                           | dog           |\n",
    "    | NNS  | Noun, plural                                     | dogs          |\n",
    "    | NNP  | Proper noun, singular                            | John          |\n",
    "    | NNPS | Proper noun, plural                              | Smiths        |\n",
    "    | PDT  | Predeterminer                                    | all the       |\n",
    "    | POS  | Possessive ending                                | 's            |\n",
    "    | PRP  | Personal pronoun                                 | I, he         |\n",
    "    | PRP$ | Possessive pronoun                               | my, his       |\n",
    "    | RB   | Adverb                                           | quickly       |\n",
    "    | RBR  | Adverb, comparative                              | faster        |\n",
    "    | RBS  | Adverb, superlative                              | fastest       |\n",
    "    | RP   | Particle                                         | up, off       |\n",
    "    | SYM  | Symbol                                           | +, %, &       |\n",
    "    | TO   | to                                               | to            |\n",
    "    | UH   | Interjection                                     | ah, hey       |\n",
    "    | VB   | Verb, base form                                  | run           |\n",
    "    | VBD  | Verb, past tense                                 | ran           |\n",
    "    | VBG  | Verb, gerund or present participle               | running       |\n",
    "    | VBN  | Verb, past participle                            | run           |\n",
    "    | VBP  | Verb, non-3rd person singular present            | run           |\n",
    "    | VBZ  | Verb, 3rd person singular present                | runs          |\n",
    "    | WDT  | Wh-determiner                                    | which         |\n",
    "    | WP   | Wh-pronoun                                       | who, what     |\n",
    "    | WP$  | Possessive wh-pronoun                            | whose         |\n",
    "    | WRB  | Wh-adverb                                        | where, when   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Now let us simulate a simple pos process, by POS processing a word tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization | pos tag -> [('Tokenization', 'NN')]\n",
      "is | pos tag -> [('is', 'VBZ')]\n",
      "the | pos tag -> [('the', 'DT')]\n",
      "process | pos tag -> [('process', 'NN')]\n",
      "of | pos tag -> [('of', 'IN')]\n",
      "breaking | pos tag -> [('breaking', 'VBG')]\n",
      "down | pos tag -> [('down', 'RB')]\n",
      "text | pos tag -> [('text', 'NN')]\n",
      "into | pos tag -> [('into', 'IN')]\n",
      "smaller | pos tag -> [('smaller', 'JJR')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "our_str = \"Tokenization is the process of breaking down text into smaller units, such as words or sentences. These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) and text analysis. It helps in understanding the structure and meaning of the text.\"\n",
    "words = word_tokenize(our_str)\n",
    "for i in range(0,10):\n",
    "    word = words[i]\n",
    "    print(f\"{word} | pos tag -> {pos_tag([word])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition\n",
    "- what is it?\n",
    "    - Named entity Recognition is the process of identifying and classifying entities into predefined categories.\n",
    "    - Entities are nothing but nouns or words similar to nouns (like Rupee, Dollar) in a sentence.\n",
    "    - Some of the common predefined categories are: \n",
    "\n",
    "        | Category       | Description                                  | Example          |\n",
    "        |----------------|----------------------------------------------|------------------|\n",
    "        | PERSON         | Names of people                              | Barack Obama     |\n",
    "        | ORGANIZATION   | Names of companies, government organizations | Microsoft        |\n",
    "        | LOCATION       | Geographical names like cities, countries    | Tokyo            |\n",
    "        | DATE           | Dates in various formats                     | January 23, 2025 |\n",
    "        | TIME           | Times of the day                             | 12:28 PM         |\n",
    "        | MONEY          | Monetary values                              | $100             |\n",
    "\n",
    "\n",
    "- 3 steps of NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us perform NER on a string.\n",
    " - Before passing the string to NER, let us perform \n",
    "    - word tokenization and \n",
    "    - POS on the tokens \n",
    "    to ease up the NER process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Honolulu/NNP)\n",
      "  ,/,\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk #ne_chunk is the module that does NER in NLTK\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Barack Obama was born in Honolulu, Hawaii.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "named_entities = ne_chunk(tagged_tokens)\n",
    "\n",
    "# Print the named entities\n",
    "print(named_entities) #entities are those words that have the category associated with it and are wrapped inside parenthesis\n",
    "#(Barack and Obama) are identified as PERSON, and (Honolulu and Hawaii) are identified as GPE - geopolitical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax\n",
    "- Each language has its own set of rules, like what part of the sentence will appear where?\n",
    "- By establishing a syntax of a language, during NLP we can clearly understand the different parts of a sentence.\n",
    "- By classifying different parts of a sentence, it would be easier for us to understand what the sentence is trying to communicate.\n",
    "- It broadens the horizon of classification done at the POS stage, by identifying parts of sentence like verb phrase, noun phrase etc.\n",
    "- Each part has a tag associated with it, just like in POS.\n",
    "- Using these tags and the tags derived during the POS stage, we can create a tree like structure from a sentence. This is called a syntax tree.\n",
    "- Let's say we have a sentence: “The quick brown fox jumps over the lazy dog”\n",
    "    - post POS, the sentence will have been tagged like this: `The (DT) quick (JJ) brown (JJ) fox (NN) jumps (VBZ) over (IN) the (DT) lazy (JJ) dog (NN)`.\n",
    "    - Now, after applying the syntax of English language, we can derive a syntax tree like this:\n",
    "        ```              \n",
    "                         S\n",
    "                        / \\\n",
    "                    NP   VP\n",
    "                    /  \\  /  \\\n",
    "                    DT  JJ JJ  NN   VBZ  PP\n",
    "                |   |   |   |    |   /  \\\n",
    "                The quick brown fox jumps over  NP\n",
    "                                            /   \\\n",
    "                                            DT    JJ\n",
    "                                            |      |\n",
    "                                            the    lazy\n",
    "                                                    |\n",
    "                                                    NN\n",
    "                                                    dog\n",
    "        ```\n",
    "    - Some of the top level tags we see are the phrase tags:\n",
    "\n",
    "    | **Tag** | **Description**                |\n",
    "    |---------|--------------------------------|\n",
    "    | NP      | Noun Phrase                    |\n",
    "    | VP      | Verb Phrase                    |\n",
    "    | ADJP    | Adjective Phrase               |\n",
    "    | ADVP    | Adverb Phrase                  |\n",
    "    | PP      | Prepositional Phrase           |\n",
    "    | SBAR    | Subordinating Conjunction + Sentence |\n",
    "    | WHNP    | Wh-Noun Phrase (e.g., what, who)|\n",
    "    | WHADVP  | Wh-Adverb Phrase (e.g., when, where)|\n",
    "    | WHPP    | Wh-Prepositional Phrase        |\n",
    " \n",
    "- Using the syntax trees, a complex AI can be developed which would be able to answer the question \"Who jumped over the lazy dog?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Chunking\n",
    "- Chunking can be assumed as a process opposite to that of tokenization.\n",
    "- Here we can assume that the tokenized words are chunked based on the syntax rules.\n",
    "- Let us see the simulation of chunking using a simple rule defined using a regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (NP quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  the/DT\n",
      "  (NP lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Get part of speech tags\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Define the chunking rule\n",
    "# This rule chunks an adjective followed by a noun (e.g., \"quick brown fox\" and \"lazy dog\")\n",
    "chunk_rule = \"NP: {<JJ>*<NN>}\"\n",
    "\n",
    "# Create the chunk parser\n",
    "chunk_parser = RegexpParser(chunk_rule)\n",
    "\n",
    "# Parse the sentence\n",
    "chunked_sentence = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print the chunked sentence\n",
    "print(chunked_sentence)\n",
    "# We can see that the phrases are chunked based on POS and the syntax (regex rule) we have applied.\n",
    "# The chunks have phrases like \"quick brown\", \"lazy dog\" - which are chunked based on our syntax rule \n",
    "# that is looking for an adjective followed by a noun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
