{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NLP using NLTK\n",
    "#### 2 parts of NLP\n",
    "    - NLU\n",
    "    - NLG\n",
    "#### Why NLU is hard?\n",
    "    - Ambiguity? 3 types of ambiguity\n",
    "        - lexical ambiguity\n",
    "        - syntactic ambiguity\n",
    "        - referential ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation and setup of NLTK\n",
    "- install NLTK `py -m pip install nltk` - This will download the basic nltk package but not does not have the necessary tools for us to perform NLP\n",
    "- For that we need to install other packages of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on executing the above function, a window will be opened with different nltk package options that can be individually downloaded.\n",
    "- To keep things simple, let us select the all packages option and click download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "- 3 basic steps involved in tokenization?\n",
    "    - strip: stripping individual words from a sentence.\n",
    "    - understand: understand importance of each word with respect to the sentence\n",
    "    - describe: produce a structural description of the input sentence \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyzing the corpus\n",
    "    - A corpus is nothing but a collection of texts\n",
    "    - NLTK leverage various kinds of Corpora (plural - corpus), and we as a developer can target and make use of the required corpus to perform NLP on our input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'bcp47.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'extended_omw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw-1.4.zip', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pe08', 'pe08.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet2022', 'wordnet2022.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk.corpus\n",
    "\n",
    "#all the corpora available to nltk\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))\n",
    "\n",
    "#the words contained in a corpus called brown\n",
    "print(nltk.corpus.brown.words())\n",
    "\n",
    "#some corpus can have subfiles in them as well\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing our own string\n",
    " - using a word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'words', 'or', 'sentences', '.', 'These', 'smaller', 'units', 'are', 'called', 'tokens', '.', 'Tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'text', 'analysis', '.', 'It', 'helps', 'in', 'understanding', 'the', 'structure', 'and', 'meaning', 'of', 'the', 'text', '.']\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "our_str = \"Tokenization is the process of breaking down text into smaller units, such as words or sentences. These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) and text analysis. It helps in understanding the structure and meaning of the text.\"\n",
    "\n",
    "our_words = word_tokenize(our_str)\n",
    "print(our_words) #you can see that even characters like ',' are also splitted into a separate word\n",
    "\n",
    "print(len(our_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- looking at frequency distribution of words using nltk's FreqDist module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 4, 'the': 3, 'text': 3, 'tokenization': 2, 'is': 2, 'of': 2, 'smaller': 2, 'units': 2, 'in': 2, 'and': 2, ...})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('.', 4),\n",
       " ('the', 3),\n",
       " ('text', 3),\n",
       " ('tokenization', 2),\n",
       " ('is', 2),\n",
       " ('of', 2),\n",
       " ('smaller', 2),\n",
       " ('units', 2),\n",
       " ('in', 2),\n",
       " ('and', 2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fd = FreqDist()\n",
    "\n",
    "for word in our_words:\n",
    "    fd[word.lower()] +=1\n",
    "\n",
    "#frequency distribution of each word\n",
    "display(fd)\n",
    "\n",
    "#we can get the top 10 frequently used words\n",
    "display(fd.most_common(10))\n",
    "\n",
    "#frequency of word and\n",
    "display(fd[\"and\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stripping paras using a blank line tokenizer\n",
    "    - so nltk has different tokenizers, that evaluates tokens based on different logic.\n",
    "    - so, similarly we have a blank line tokenizer, that considers any sequence of blank lines as a delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import blankline_tokenize\n",
    "\n",
    "our_paras = \"\"\"\n",
    "Tokenization is the process of breaking down text into smaller units, such as words or sentences. These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) and text analysis. It helps in understanding the structure and meaning of the text.\n",
    "\n",
    "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "\n",
    "Text analysis involves various techniques to extract information and insights from textual data. These techniques include tokenization, stemming, lemmatization, part-of-speech tagging, named entity recognition, and sentiment analysis. By applying these techniques, we can gain a deeper understanding of the content and context of the text.\n",
    "\"\"\"\n",
    "our_blanks = blankline_tokenize(our_paras)\n",
    "print(len(our_blanks)) #No of blank lines or we can understand them as paras\n",
    "print(our_blanks[1]) #First para\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization via Bigram, Trigram, Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "- porterStemmer, lancasterstemmer, snowballstemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "- wordnet lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "- import and list stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of speech\n",
    "- Chart of tags and their descriptions\n",
    "- show the tags of a sample string using nltk.pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition\n",
    "- what is it?\n",
    "- 3 steps of NER\n",
    "- Performing NER on a tokenized, stemmed, and pos-tagged string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax\n",
    "- syntax tree\n",
    "- chunking\n",
    "    - simulation of chunking using a simple regex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
